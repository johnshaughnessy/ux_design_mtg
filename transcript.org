* transcript
** Greg: We have a few things to figure out.
These 3DOF controllers are in a couple categories:
- Ones that have a trigger
- Ones that don't have a trigger
And a third category, if you like:
- Magic leap has a trigger and a non-depressable touchpad.
*** utterance : https://www.jonicoma.com/ux_design_mtg/utterances/greg-we_have_a_few_things_to_figure_out.mp4
*** raw : https://www.jonicoma.com/ux_design_mtg/utterances/john_perspective_raw.mp4
** Dom : Oh my god, are you serious?
*** utterance : https://www.jonicoma.com/ux_design_mtg/utterances/dom-omg_are_you_serious.mp4
*** raw : https://www.jonicoma.com/ux_design_mtg/utterances/john_perspective_raw.mp4
** Greg : Yes
*** utterance : https://www.jonicoma.com/ux_design_mtg/utterances/greg-yes.mp4
*** raw : https://www.jonicoma.com/ux_design_mtg/utterances/john_perspective_raw.mp4
** Dom : Does it have a trigger though?
*** utterance : https://www.jonicoma.com/ux_design_mtg/utterances/dom-does_it_have_a_trigger.mp4
*** raw : https://www.jonicoma.com/ux_design_mtg/utterances/john_perspective_raw.mp4
** John : You can still click it.
*** utterance : https://www.jonicoma.com/ux_design_mtg/utterances/john-you-can-still-click-it.mp4
*** raw : https://www.jonicoma.com/ux_design_mtg/utterances/john_perspective_raw.mp4
* rest

Kevin : Nonono it's still clickable.

John : It's just not depressing.

Kevin : Yea. It's tapped.

John : It knows when you tap it. It's not ...

Dom : It knows when you tap it or when you press it? That's different.

Greg : Yea. There's one less degree of freedom though right because 
you have all of those on the Oculus Go controller but you're missing 
one now on the Magic Leap.

Kevin : Uh, without knowing the API, it could be pressure-sensitive.
So like, it can differentiate that.
If it's the way I expect it to be, it's like iOS, where you have haptic 
push and press. Like it knows the difference between a press and a tap.


Dom (in despair) : Oh no, did I not charge my Oculus Go?


Greg : Anyway, I guess my point was that... 
We don't need to worry about [Magic Leap] right now.
I was just throwing that out there as a fun little thing to worry about later.
But we do need to figure out the trigger vs non-trigger case because the 
daydream platform doesn't have a trigger, right?

John : Yea.

Kevin : That's correct.

<Dom exits>

Greg : Looks like we lost Dom.

John : He didn't charge his Go, I think he'll probably just come back with his laptop in a second.

Greg : Okay.
It's weird I'm getting lower framerate now.
I wonder what that's about.
Anyway so, I think the carving point or whatever you want to call it -- why we 
want to split [trigger vs non-trigger] up is the case that Kevin's worried about,
with the "grab and release, but also then use" case.

<Dom enters>

Kevin : No it's just that we're trying to create more axes of freedom for the input
on that controller. Ideally we want to have as many options as possible. So, 
for that example ["grab and release, but also then use"], I wanted to use face buttons
for changing color and changing size of the pen. Left-and-right work fine right now but 
up-and-down really don't.
Actually, I'm not even doing left and right because left-and-right is reserved for 
rotating. So, you can press left-and-right and you'll ratchet turn. We can do 
a tap on left-and-right, but we don't have a good way of knowing the difference 
between a press and a tap.
We could do a thing where if you press, then a tap won't register and if you tap 
then you better not press or else it'll count as a press. But anyway, the point 
was that we have very limited things we can do on that platform, especially 
because one of the buttons is reserved for the browsers.

Greg : You mean on daydream or ...?

Kevin : I meant Oculus Go-- I haven't done anything on daydream.
But on all the platforms, there's a menu button that is reserved.
We have very limited input on Go or Daydream.

Greg : The thing I'm talking about first is the [pickup-and-hold] interaction
with the pen on 3DOF devices.

Kevin : It works on Go and probably gear also (although I haven't tested it) 
because the input is set up exactly the same way. You have a trigger and a 
touchpad. I've introduced the concept that you have a [primary] and a [secondary]
action. Both can pick up a thing, but only a [primary-release] can drop a thing 
after that initial grab for the pen, for example. For [normal objects], they'll 
act in exactly the same [as it they do now].
So the idea is that for those platforms like the Oculus Go, the trigger is considered
[secondary] and the touchpad is considered [primary]. So if you pick up a pen, 
you can pick it up with either the trigger [secondary] or the touchpad [primary],
draw with the trigger [secondary], and then drop it with the touchpad [primary].

Greg : Ok, that makes sense.

Kevin : And the pen is locked to your hand in that process. So when you pick it up
with either, it'll stay attached to your hand until you press the touchpad [primary-release]
(potentially for the second time).
For [normal object] interaction, it does not do that. It just lets you pick it up 
and drop it immediately.

Greg : Right, it just doesn't stick to your hand, basically.

<Jim enters>

Greg : Oh hey, Jim
Jim : What's up! I didn't want to interrupt.

Greg : What are your thoughts [Kevin], on the way that you'll change the settings 
on the pen. So you've picked up the pen, it's in your hand. You have no fingers 
down on any of the buttons at this point. What's your method for changing size and
color at this point?

Kevin : So on 6DOF, e.g. on the vive, if you scroll up and down and get to the 
max scroll distance or min scroll distance right now it changes color.
So you can scroll the pen all the way to you and once you go "past" the minimum 
distance it will just change color. Right now all that's doing is firing this 
new event that says, "Hey you've scrolled". On the Vive also, if you swipe left 
or right, it will change the scale. So it's just relying on [vertical] and [horizontal]
scroll events, basically.

Greg : Is that how you're imagining it'll work on the Oculus Go 3DOF controller? 
You'll just touch across the surface of the scroll pad to change the state of the 
pen (in terms of size and color)?

Kevin : I haven't tried. Like I said the only thing I've done is allowing you to 
change color by scrolling. I was not super confident on.... 
Actually, in fact in the 6DOF case that scrolling only works on the left controller 
because we use [left-and-right] on the right vive controller for ratchet-rotating.
We don't have good way right now to differentiate between a push-down on the touchpad
and a tap or swipe-push on the touchpad.
Maybe now that I'm doing it on the scrolling, we'll be able to differentiate between
ratchet rotating on push-down and changing pen size via swiping, but I want to avoid
false positives where you change the scale of the pen while scrolling or vice versa.

Greg : I see. But that is basically not an issue with the [overall model], it's 
just an implementation detail that it's a little unclear whether we'll be able 
to detect [that difference].

Dom : I mean, it's still kind of a little bit overloading happening with that 
functionality. There will be a technical challenge in detecting false positives 
and that sort of stuff but also it just is a lot [of functionality] mapped onto 
one thing. Admittedly, we don't have much choice on Oculus Go right -- a trigger 
and a touchpad is all we have.

Jim : I guess the question I have is that some apps that have drawing tools 
or similar things will do something like, "click one of the face buttons" to 
open up the menu that lets you pick colors, or change brush size. Is that 
within the realm of possibilities for us? Do you guys think that is a good idea
to eliminate some of the false positives?

Dom : I feel like menus are inevitable for us. Let's think about our most 
limited platform. We have one controller on Oculus Go. Actually, the most 
limited platform is Daydream. We should really discuss if we're supporting 
controllers without triggers or not.
I don't see a way to get away with all the functionality we want to allow 
without having some sort of menu system, whether it's just the pause thing or 
some other kind of context menu that triggers when you lift the controller
or something else.

John : Yea I want to piggy-back on that and separate two kinds of concerns 
that Kevin has brought up with the drawing stuff. When we have one button 
on the daydream or a trigger and a touchpad on Oculus Go, one concern is 
(as we said) we need to make maximum use of those limited inputs. You know 
[swipe right] is recognized differently than [pressing down on the right side].
That's one set of concerns, which is "Making the most of what limited buttons 
you have."
The second concern is that if we have 10 actions that we want to allow the user
to perform, maybe we can get away with the way we're handling actions right now but 
as soon that number of actions increases (and I think we're hitting this point 
now for daydream and on Oculus Go) we're going to need to take these learnings 
around action sets and different modes that you're operating in at any given 
time and actually apply it to our app. Along those lines, like what Jim was 
saying, I could see that menu or key sequence or whatever it is to change brush
size or change color being how we introduce these concepts to Hubs.

Jim : I brought it up thinking that Kevin has put a lot of work into the drawing
tool and I wouldn't want us to limit how cool that tool could get because we 
don't have enough buttons for it. Or that it might mess us ratchet turn or something.
Like maybe if it's menu-driven then sure you can do all sorts of things like 
"now it can do particles".

Dom : I also want to have a framework for when we need to add something like this 
we know how. Not just technically -- like whatever library we use. What is our 
design language for talking about it? Right now we're just kind of randomly, 
haphazardly deciding on this stuff as we create it like 
- "Ok, we need to play and pause a video." 
- "Ok, you can click on it to play it or pause it."
 
- "Ok, we need to change the color"
- "Ok, yea you scroll"
We're just haphazardly adding these things.

Jim : It's ad-hoc.

Dom : Yea, and we need a way to... when we're going to add a new feature, it's 
not even a question like we know. It's like 
- "Ok we need to do X",
- "Ok yea, that's obviously going to be in a menu because 'that's-how-we-do-
features-like-X'"

John : Yea piggy-backing off that I think we have enough ideas that have been 
floated out in previous discussions to start prototyping an alternative input-
handling scheme. I have some concerns with trying to make those changes. They're 
not small changes because input ends up touching lots and lots of parts of the 
app and it's really hard to say "Ok we're going to change how the cursor works"
for example. The cursor interacts with a lot of things.
But one question I have that we have yet to answer in our previous discussions 
is, "What's the difference between an action set (which, we may have multiple 
active at a time, like when you point at something that's scrollable your action
sets change such that you have a scrolling action set in your active action sets),
... What's the difference between an action set and key sequences? When I think 
about something like emacs, so much of the versatility comes from the fact that 
you can redefine a key sequence to mean some action in the app, and then you 
can switch modes to activate those key sequences which are very analogous 
to our action-set concepts. I don't think we've ever talked specifically about
key-sequences which I think we can talk about interchangably with the way Dom's 
been saying menus. Like, navigating a menu to select an action is essentially the 
same thing as performing the right key sequence for that action.

Dom : And the kinds of menus I'm envisioning are potentially like gesture-based 
menus where you can learn a gesture-sequence. If you imagine a radial menu where 
once you see it, you can then go [up], and then you go [left], and then you go 
[down]. That motion of going [up]-[left]-[down] can become muscle memory and you 
can perform actions without looking at the menu. That's the KIND of thing I'm 
imagining.

Greg : I'm having a hard time trying to figure out if now is the right time to 
introduce this stuff. Given our current set of features, and the gaps in our 
UX and the arrival of the pen tool, I don't feel that pain. I really feel like 
the pen that Kevin has is introducing another form of modality in the app which 
is namely that you're either holding it or you're not right, so if you're 
holding it, now you're in a different mode, so we can actually remap a significant
percentage of the controller while you're drawing. That gives us a bit of the 
lever to stave off the need for like a wholistic UX for meta-controls. Don't 
forget that we also have the pause mode, so like you said Dom there's a significant
amount of these things that we'll be able to address using that metaphor we 
already have. I want to make sure we don't --

<Jim loads a model>

Greg : oh--
Jim : uh-- 
Greg : It just dropped me out of VR, let me try to come back in.
Jim : It's tiny and I can't see it. I wonder what...
Greg : I just lost my hand but...
Kevin : Nononah, sounds like 

Greg: I guess if there's a strong motivating reason why this is not going to be 
possible to support then we can see if we can find a way to come up with a design
pattern for this stuff but like, I imagine there's two major gaps right now, to me.
- You don't have the ability to scale and rotate on a subset of the device platforms.
- We need a way to make simple changes to the pen tool on all of them so it's strictly
like color and size.

Dom : I mean I think there's still other things like we need to be able to control 
the volume. Also the way we're playing and pausing videos right now is just completely
broken and shitty. 

Greg : How does it work right now?
Dom : Right now you can click a video to play and pause it.
But then that means when you pick it up to move it you also pause it. 
It's just completely overloaded on that. There's no way to set volume 
right now, but you're going to want a way to set volume. 

Jim : What if it's a duplicate of the original video? Does it also pause all of them?
Dom : No, right now they're treated as separate videos completely.

Kevin : Ok so,

Dom : PDF's have another concept added where there are these floating buttons on the 
page to go to the [next] and [previous] pages, which is only shown to the owner. 
These all work, they're just completely ad-hoc. Every single thing you want to interact
with is a completely new modality to learn. It's just kind of weird. 

Jim : That does kind of speak to the merits of coming up with a universal schema.

Greg : Yea, I don't know whether these all fit under the same thing though right because 
we're talking about a couple different concepts here so we have on-object interactions
including things like paginating PDFs and [play][pause] and [volume] right? So we already
have a metaphor for that, it's pause mode. That might not be the right UX because of things like
slides.

Dom : Right that's why I only show those buttons to the owner. 

Greg : What I'm getting at though is that I don't know if there's any new universal design 
language that we can both solve some of these issues and solve the issues we're also 
talking about which is like 3D object interaction and then the tool controls. 

Dom : The pen tool one sounds very related to the video controls / paging, all that stuff.
That sounds very related. 3D object manipulation is maybe different, but I'm not sure.

Kevin : So I think I've made pretty good strides for 3D object manipulation going on into
the future. Once we get input mappings figured out (however the hell we're doing that with
switching mapping or whatever, this will translate nicely), any object can be considered 
an interactable in some way. We have no easy way to set state on that object based on the 
input. So we can drive things like changing the color of the pen. Or changing volume on a 
video. I don't think it's complicated. I don't think it's hard. 
Now if we take a step back up, the problem is that we are very, very limited on input on 
certain devices, and that's really where we break down right now. It's the hardest thing 
that I've experienced where on certain devices it's just not possible to really get what 
we want. 
My gut says that the easiest way to work around that is to have menus or buttons that we 
can press in VR or whatever to be able to do those action. Having tried the [Magic Leap]...
Their creation app lets you spawn an object from a menu and you can drop it into the world
and that object is going to have default behaviors e.g. whether it's going to respect 
gravity or not. If you spawn a ball, the ball is just going to spawn in the area. If you 
go back to that menu (you basically just have a trigger and a button that spawns a menu -
that's it)... If you open that menu, and you're going to be given options to either go 
into freeze mode --

Jim: -- or delete mode --

Kevin : Click on the freeze mode button, and now when you pick up that ball again, it'll
just stay in place until you move it again. So that's one example. If you want to delete 
something that's doing the same thing.

Dom : I mean, they also have 6DOF input though, right? 

Jim : One handed.

Kevin : Uh,

Dom : Yea one handed but that solves the position/rotation problem because you can 
do that via [direct manipulation]. 

Kevin : But I'm saying they only have the one trigger to actions basically and 
then a button to open the menu.

Greg : So they basically take a model where you pre-emptively tell it what you 
want you next trigger action to do, as a verb, and then you do it, and repeat.

Kevin : Right. Something similar we could consider for example (I'm not suggesting 
we do this; it's just an idea) is that we could press a button, it pulls up 
a menu of some sort, and then you select a rotation tool. Then when you click 
on the next object, instead of your motion now moving the object, it's going 
to rotate it based on some axes. 

John : Yea I think what you're talking about is co-opting the visuals which 
we have infinite freedom of expression with and like a pointer or swiping or 
some action on the controller. Since we don't have a button on every action 
we might have on the keyboard that the user wants to perform, we show the 
user some stuff and then give them a nice way to indicate which of those 
things they want to do. 

Jim : So you're picturing -- so for example if I have this duck and it was 
sitting out there like that, and I went into my little menu and I hit like 
[rotate], now I can rotate it with my 3DOF controller's 3DOFs.

Kevin : Scale would be like if you move your hand up when you're holding 
the thing it gets bigger or something, whatever. IDK what that exactly 
looks like but I think it's something we need to consider because we're 
just going to have areas where we're going to have limited input --

Jim : -- there's going to be more, too. Change the color of it, well I need
a thing for that -- 

Kevin : If we avoid going down this route of having [menus], it is nice in 
that we don't have to deal with menus but I think we're going to have problems
with people understanding what's available to them to be able to do, in 
addition to people not being able to do things on certain platforms. If we
do this we can have a universal (works on every platform) things the user 
can do, and we can layer stuff on top of that (this is not prohibiting you 
from, with 6DOF, just grabbing a thing and rotating it because obviously 
you can do that, right), but like, that can just be layered on top of the 
system. That way any system you know from any input device, you know that 
you have this base level functionality you can do.

John : Yea I'd go so far as to say the menu that's close to your person is
equivalent to pointing at a thing and showing on-object interactions as 
Greg said earlier. Those two are the same concept I think, which is, 
"Show the user a thing and let them point at it or indicate it somehow"

Jim : It's like a tool palette like in photoshop. I have the select tool 
and now I'm selecting things.

Dom : It's more akin to like the context menu right, it's more like 
right-clicking something.

Jim : Sure.

Kevin : Well, however that works right. We either have to think about 
what that menu means or where that menu is. I know Greg has concerns with 
like going too far into the realm of what we had at Altspace where like 
there's this radial where when you click on it all these other options 
pop up and it's always there and there's a billion different things, but 
then how do we make it context sensitive or how do we make it... Maybe this
ties into state in the application so it knows that 
"Oh, when you clicked on this object now you are in this object-interaction state 
and we know if you push a certain button then it pulls up the menu or something.
We have to figure out what that means."

Jim : Right, and we have to figure out what that means. Do we highlight 
objects to indicate what's active.

Greg : We have a lot of stuff in here now and we already have some menu 
concept which is the pause mode which was a way for us to hide 
incidental complexity.

Dom : Although we obviously need to work on the discoverability of that 
because no one knows that you can delete objects and like I don't think 
discoverability is necessarily the most important thing like I honestly 
don't really care that much about 

Jim : What if it's -

Dom : - first time user experience as much as that's usually like, "Oh-

Greg : Well actually the discoverability of the pause mode itself 
was good from what I understand.

Dom : Right, well people just didn't understand what it was for.

Greg : That's a solvable problem though I think for sure. 
What I'm saying is not really contradicting this, but what I'm getting at
and the thing that gets me a little nervous is that the more steps 
and the more layers of indirection we have for some of these fundamental
things like scaling and moving, the more collatoral damage we'll cause 
them unless we ensure there's a natural and intuitive interaction 
that doesn't require these abstractions to get in the way because 
a lot of this stuff is probably going to be about flow and people communicating 
fluidly, and so I really like the model we have now because like 
you can become a -- it's limited, right like I don't know how to scale
for example on 3DOF but I know how to do the operations that I can 
do pretty well and they're like really intuitive to me and like 
they're muscle memory at this point. I can pick up that duck over 
there and start tossing it around and moving it towards me and 
placing it in a specific spot, pretty darn fluidly without thinking 
about it now and I want us to be careful not to break that. 

Dom : Yea, and I agree because even you look at some simple things
like muting and unmuting yourself is very -- not that --. Uh, it's 
a pain in the ass. It's an action where I have to like completely 
stop what I'm doing, stop my train of thought, aim at the hud 
think about it, click on it, say my thought, and then if I want to
mute myself again I have to click on it again. And then I have to 
do it again,

Jim : And remember that the button is up there

Dom : Contrast this with the way I mute myself when I use my laptop,
I just have the n key, I can just hit it, it's like on and off 
I don't even have to think about it, and I'm literally just hitting
it, as a push to talk. I'm just toggling it. 

Greg : mhm

Dom : That is not true of muting in hubs. That's the concern with 
menus or on-object buttons or anything you put in the pause menu 
is that like you completely break flow to go do those things ...

Kevin : -Ok, let me reiterate 

John : - yea

Kevin : that I'm not suggesting that those be the only way of 
doing things.


